{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ad7f83-a8ec-482b-b766-aa9cd2eca117",
   "metadata": {
    "id": "91ad7f83-a8ec-482b-b766-aa9cd2eca117",
    "tags": []
   },
   "source": [
    "# Notebook 5: Adversarial Examples\n",
    "\n",
    "In this notebook we'll explore __adversarial examples__, an interesting phenomenon in which neural networks can be \"fooled\" with small changes to their inputs. We'll learn how to craft adversarial examples to fool both image-domain and audio-domain models, and explore ways in which models can be made more robust against these types of attacks.\n",
    "\n",
    "The notebook is broken up as follows:\n",
    "\n",
    "  1. [Setup](#setup)  \n",
    "  2. [What Are Adversarial Examples?](#intro)  \n",
    "     2.1 [A Simple MNIST Classifier](#mnist)  \n",
    "     2.2 [Crafting an Adversarial Example](#craft)  \n",
    "  3. [Audio-Domain Attacks](#audio)  \n",
    "     3.1. [A Simple AudioMNIST Classifier](#audiomnist)  \n",
    "     3.2  [Crafting an Audio Adversarial Example](#craft-audio)  \n",
    "  4. [Building Robust Models](#robust)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410246cc-3cb0-47a0-8578-d4f808c11611",
   "metadata": {
    "id": "410246cc-3cb0-47a0-8578-d4f808c11611",
    "tags": []
   },
   "source": [
    "## __1.__ <a name=\"setup\">Setup</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd3440e-b8f0-45cc-8dec-368a72e7cf68",
   "metadata": {
    "id": "8fd3440e-b8f0-45cc-8dec-368a72e7cf68"
   },
   "source": [
    "Make sure the needed packages are installed and utility code is in the right place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a6a47c-2280-46a4-95c5-df7496acbb67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28a6a47c-2280-46a4-95c5-df7496acbb67",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.adversarial_examples import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c508d427-eb72-472d-b5c0-475a29b38914",
   "metadata": {
    "id": "c508d427-eb72-472d-b5c0-475a29b38914"
   },
   "outputs": [],
   "source": [
    "# download AudioMNIST dataset in tensor format (this will save time)\n",
    "!mkdir ./data/\n",
    "!cd ./data/ && mkdir AudioMNIST\n",
    "\n",
    "url_x = \"https://drive.google.com/uc?id=1FdLxBSTaH6TuMvBA-wAs-4_kjbEfRSuj\"\n",
    "url_y = \"https://drive.google.com/uc?id=1iMkck7iULEll1HUp_iaYX6rm4LV9mkp6\"\n",
    "out_x = \"./data/AudioMNIST/audiomnist_tx.pt\"\n",
    "out_y = \"./data/AudioMNIST/audiomnist_ty.pt\"\n",
    "\n",
    "gdown.download(url_x, out_x, quiet=False)\n",
    "gdown.download(url_y, out_y, quiet=False)\n",
    "\n",
    "# AudioMNIST dataset raw download - only do this if gdown fails, as it is SLOW\n",
    "# %cd ../data\n",
    "#!git clone https://github.com/soerenab/AudioMNIST.git\n",
    "# %cd ../code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca343cd-c8ed-4e9f-b22b-58ad287e6a42",
   "metadata": {
    "id": "eca343cd-c8ed-4e9f-b22b-58ad287e6a42",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e",
    "tags": []
   },
   "source": [
    "## __2.__ <a name=\"intro\">What Are Adversarial Examples?</a>\n",
    "\n",
    "__Adversarial examples__ are inputs to a machine-learning model that have been __perturbed__ (modified), often imperceptibly, so that the model makes an incorrect prediction. The image below, from [Goodfellow et al. (2015)](https://arxiv.org/pdf/1412.6572.pdf), illustrates how an __adversarial perturbation__ can be added to picture of a panda to fool a vision classifier.\n",
    "\n",
    "<br/>\n",
    "<center>\n",
    "<img width=\"700px\" src=\"https://openai.com/content/images/2017/02/adversarial_img_1.png\"/>\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "While the adversarial perturbation above looks like random noise, it has actually been carefully crafted to modify this specific image. Recall from the previous notebook that we can think of an image as a point lying in a high-dimensional space. An adversarial perturbation can be thought of as a small vector in this space that we add to our image:\n",
    "\n",
    "<br/>\n",
    "<center>\n",
    "<img width=\"700px\" src=\"https://drive.google.com/uc?export=view&id=1U-nud0clJMZ76EAcM06vBYL1CQcU5WBl\"/>\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "To find effective perturbation vectors in the input space, we'll use the __gradients of the machine-learning model we wish to fool__. To keep these perturbations relatively imperceptible, we'll constrain the magnitude of our perturbation vector in terms of an __$L_p$ norm__:\n",
    "\n",
    "$$\\|\\delta\\|_p  = \\left( \\sum_{i = 1}^{d} \\lvert \\delta_i \\rvert^p \\right)^{\\frac{1}{p}} $$\n",
    "\n",
    "where $\\delta = \\{\\delta_1, \\dots, \\delta_d\\}$ is a $d$-dimensional perturbation vector. For example, the $L_2$ norm ($p = 2$) corresponds to the Euclidean norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89d3c27-e4a4-49f0-ae5a-1602b72ca2f5",
   "metadata": {
    "id": "d89d3c27-e4a4-49f0-ae5a-1602b72ca2f5",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e",
    "tags": []
   },
   "source": [
    "### __2.1__ <a name=\"mnist\">A Simple MNIST Classifier</a>\n",
    "\n",
    "Before we can begin crafting adversarial examples, we need a classifier to fool. Here, we will download the MNIST dataset and load a pre-trained convolutional neural network to perform classification. Without any adversarial meddling, the network achieves 99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e942b0-094c-4de8-b792-817834b393e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3e942b0-094c-4de8-b792-817834b393e3",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)  # fix random seed\n",
    "\n",
    "# select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# download MNIST data\n",
    "mnist_train, mnist_test = load_mnist()\n",
    "\n",
    "# load neural network classifier\n",
    "model = MNISTClassifier()\n",
    "model.to(device)\n",
    "\n",
    "# uncomment this to train a model yourself! Beware, this can take a while on CPU\n",
    "# train_mnist(model, device, mnist_train, mnist_test)\n",
    "\n",
    "# load pretrained weights (included with repo)\n",
    "model.load_state_dict(torch.load(\"./utils/adversarial_examples/pretrained/mnist_classifier.pt\", map_location=device))\n",
    "\n",
    "# print model summary\n",
    "print(summary(model, (1, 28, 28)))\n",
    "\n",
    "# evaluate model\n",
    "test_mnist(model, device, mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974cfea-53a4-4e19-971b-8af70a5e29d4",
   "metadata": {
    "id": "a974cfea-53a4-4e19-971b-8af70a5e29d4",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "Given an input image, our model produces a score for each class (digit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23287fa-acaa-4b8b-b35f-5ea58cbd136c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b23287fa-acaa-4b8b-b35f-5ea58cbd136c",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "# select an input\n",
    "batch_x, batch_y = next(iter(mnist_test))\n",
    "x, y = batch_x[0].to(device), batch_y[0].to(device)\n",
    "\n",
    "# corresponding image & predicted class scores\n",
    "plot_mnist(x, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad8e8e6-dbc9-4832-9dad-4e6b8dfd6db9",
   "metadata": {
    "id": "9ad8e8e6-dbc9-4832-9dad-4e6b8dfd6db9",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e",
    "tags": []
   },
   "source": [
    "### __2.2__ <a name=\"craft\">Crafting an Adversarial Example</a>\n",
    "\n",
    "Image adversarial examples often take the form of small pixel-by-pixel modifications, designed to go unnoticed while changing a neural network's predicted class. Often, an adversary wants to cause a model to predict a specific class; we call this a __targeted__ attack. However, it isn't always obvious a priori how to modify an input to achieve a specific misclassification. For example, adding small pixel-wise random noise may not alter predictions, or may do so unpredictably.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc08c6c3-4e03-4cc8-92af-7e5b08652586",
   "metadata": {
    "id": "bc08c6c3-4e03-4cc8-92af-7e5b08652586",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "#### Random Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57df90fe-5e0a-4cbe-9d39-5d41ea84c492",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57df90fe-5e0a-4cbe-9d39-5d41ea84c492",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "# add random noise\n",
    "delta = torch.rand_like(x) * 1.0\n",
    "plot_mnist(x + delta, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f1cf4a-4e74-4d69-be80-914323e98b20",
   "metadata": {
    "id": "d7f1cf4a-4e74-4d69-be80-914323e98b20",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "We can construct a perturbation more carefully by leveraging gradient information from our (differentiable) neural network model. When we train a neural network, we perform gradient-based updates on its parameters to guide its output to a desired state (as measured by a classification loss function). Similarly, we can perform gradient-based updates to an _input_ in order to guide the network's output without modifying its parameters. Let $y$ be a __target__ label we want our model $f$ to predict given an input $x$ and adversarial perturbation $\\delta$. We can optimize our perturbation by minimizing the objective\n",
    "\n",
    "$$l(f, x, \\delta, y) = NLL\\left(f(x + \\delta), y\\right)$$\n",
    "\n",
    "as a function of $\\delta$, where $NLL$ is a negative log-likelihood (cross-entropy) classification loss. As we update $\\delta$ and bring the model's prediction $f(x + \\delta)$ closer to $y$, this loss will decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a583db8d-a15d-4f70-a8af-55d0e30b0e17",
   "metadata": {
    "id": "a583db8d-a15d-4f70-a8af-55d0e30b0e17",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "#### Gradients at the Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a73971-1a14-426b-b0f8-e1a04e45d831",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50a73971-1a14-426b-b0f8-e1a04e45d831",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "# pick a class we want the network to predict given our adversarial input\n",
    "target = torch.tensor([9]).to(device)\n",
    "\n",
    "# our input image; make sure we tell PyTorch to track its gradients\n",
    "x = x.reshape(1, 1, 28, 28).requires_grad_(True)\n",
    "\n",
    "# clear gradients\n",
    "x.grad = None\n",
    "\n",
    "# do not compute gradients for model parameters - only the input\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.eval()\n",
    "\n",
    "# pass through model and compute loss: how \"far\" is prediction from target?\n",
    "outputs = model(x)\n",
    "loss = torch.nn.functional.nll_loss(outputs, target)\n",
    "\n",
    "# compute gradient of loss w.r.t. our input\n",
    "loss.backward()\n",
    "\n",
    "# plot gradient\n",
    "grad = x.grad.detach().cpu()\n",
    "plt.title(\"Gradient\")\n",
    "plt.imshow(grad.reshape(28, 28).numpy(), cmap=\"gray\")\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b61bb6-b2ba-4479-8da2-fb4fcb2a42c6",
   "metadata": {
    "id": "a2b61bb6-b2ba-4479-8da2-fb4fcb2a42c6",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "We can iteratively update our input using these gradients to craft an adversarial example that minimizes our classification loss. In its most basic form, a gradient-based attack might look like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3e1c5-6c05-4e6d-8256-ac51ea238cd0",
   "metadata": {
    "id": "16b3e1c5-6c05-4e6d-8256-ac51ea238cd0",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "#### Simple Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1240685-f6c1-4308-a1c3-1eaa808224d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1240685-f6c1-4308-a1c3-1eaa808224d0",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "# pick a class we want the network to predict given our adversarial input\n",
    "target = torch.tensor([9]).to(device)\n",
    "\n",
    "x = x.reshape(1, 1, 28, 28).detach()\n",
    "\n",
    "# iteratively optimize an additive perturbation to our input\n",
    "delta = torch.zeros_like(x).requires_grad_(True)\n",
    "\n",
    "# perturbation \"training\" loop\n",
    "for i in range(1, 100):\n",
    "\n",
    "    # clear gradients each step\n",
    "    delta.grad = None\n",
    "\n",
    "    # compute model's class scores given perturbed input (x + delta)\n",
    "    outputs = model(x + delta)\n",
    "\n",
    "    # log current prediction\n",
    "    if not i % 10:\n",
    "        print(f\"Iteration: {i}; Prediction: {outputs.argmax()}\")\n",
    "\n",
    "    # compute loss: how \"far\" is prediction from target?\n",
    "    loss = torch.nn.functional.nll_loss(outputs, target)\n",
    "\n",
    "    # compute gradient of loss w.r.t. our perturbation\n",
    "    loss.backward()\n",
    "    grad = delta.grad.detach()\n",
    "\n",
    "    # update our perturbation to descend the loss\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # update with raw gradients\n",
    "        delta -= grad\n",
    "\n",
    "# plot final prediction\n",
    "plot_mnist(x + delta.detach(), y, model)\n",
    "\n",
    "# plot perturbation (difference between benign and adversarial inputs)\n",
    "plt.title(\"Perturbation\")\n",
    "plt.imshow(delta.detach().cpu().reshape(28, 28).numpy(), cmap=\"gray\")\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4415bb41-e6be-4e1b-af62-b1b146d23f48",
   "metadata": {
    "id": "4415bb41-e6be-4e1b-af62-b1b146d23f48",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "We've successfully modified our input to achieve our target label! Notice that our perturbation is concentrated around the center of the image, where the most salient features presumably lie. Alternatively, to try and \"spread\" our perturbation uniformly over the image, we might regulate the allowed perturbation magnitude at each pixel. One way to do so is by clipping the perturbation and taking uniform-magnitude steps along the gradient. This is referred to as an $L_\\infty$ __Projected Gradient Descent (PGD)__ attack: clipping can be considered a projection of the perturbation onto the $L_{\\infty}$ norm ball centered at the origin, or equivalently of the adversarial input onto the $L_\\infty$ norm ball centered at the original input. That is, clipping contrains the $L_\\infty$ norm of the perturbation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eb1a3b-3b32-4803-ad1f-68480f92a299",
   "metadata": {
    "id": "08eb1a3b-3b32-4803-ad1f-68480f92a299",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "#### $L_\\infty$ PGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce068b-5f70-41e2-be26-6da1635152c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdce068b-5f70-41e2-be26-6da1635152c2",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "# pick a class we want the network to predict given our adversarial input\n",
    "target = torch.tensor([9]).to(device)\n",
    "\n",
    "x = x.reshape(1, 1, 28, 28)\n",
    "\n",
    "# iteratively optimize an additive perturbation to our input\n",
    "delta = torch.zeros_like(x).requires_grad_(True)\n",
    "\n",
    "# \"training\" loop\n",
    "for i in range(1, 100):\n",
    "\n",
    "    # clear gradients each step\n",
    "    delta.grad = None\n",
    "\n",
    "    # compute model's class scores given perturbed input (x + delta)\n",
    "    outputs = model(x + delta)\n",
    "\n",
    "    # log current prediction\n",
    "    if not i % 10:\n",
    "        print(f\"Iteration: {i}; Prediction: {outputs.argmax()}\")\n",
    "\n",
    "    # compute loss: how \"far\" is prediction from target?\n",
    "    loss = torch.nn.functional.nll_loss(outputs, target)\n",
    "\n",
    "    # compute gradient of loss w.r.t. our perturbation\n",
    "    loss.backward()\n",
    "    grad = delta.grad.detach()\n",
    "\n",
    "    # update our perturbation to descend the loss\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # take uniform steps along the gradient for all pixels\n",
    "        delta -= torch.sign(grad) * 0.1\n",
    "\n",
    "        # bound the magnitude of the perturbation at each iteration\n",
    "        delta.clip_(min=-0.4, max=0.4)\n",
    "\n",
    "# plot final prediction\n",
    "plot_mnist(x + delta.detach(), y, model)\n",
    "\n",
    "\n",
    "# plot perturbation (difference between benign and adversarial inputs)\n",
    "plt.title(\"Perturbation\")\n",
    "plt.imshow(delta.detach().cpu().reshape(28, 28).numpy(), cmap=\"gray\")\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e22db-ce47-4420-ae18-aa4bdf73cdaf",
   "metadata": {
    "id": "bf0e22db-ce47-4420-ae18-aa4bdf73cdaf",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "This looks a little better! Note that we're not limited to considering the $L_\\infty$ norm; we could also perform an $L_2$ PGD attack, replacing our clipping operation with simple Euclidean normalization of the perturbation and dropping the `sign` operation. By now, it's hopefully clear that there are numerous ways to tweak our attacks: we could run for more iterations in the hope of finding a better perturbation, or scale our updates to adjust our effective \"learning rate\". Often, these tweaks will __trade-off between the effectiveness of our attacks in fooling the network and their perceptibility__.\n",
    "\n",
    "A final tweak to consider is incorporating magnitude constraints into the loss function via a __Lagrangian relaxation__. The idea is to penalize the magnitude of our perturbation (as measured by an $L_p$ norm) rather than constraining it explicitly. We can think of such an attack as optimizing two losses in tandem: $l_\\mathrm{adv}$, which measures our success in fooling the network, and $l_\\mathrm{aux}$, an auxiliary loss which imposes constraints on the perturbation. For input $x$, target label $y$, perturbation $\\delta$, and neural network $f$, we can write this as\n",
    "\n",
    "\\begin{align}\n",
    "l(f, x, \\delta, y) &= l_\\mathrm{adv}(f(x + \\delta), y) + \\lambda \\cdot l_\\mathrm{aux}(\\delta) \\\\[3mm]\n",
    "&= NLL(f(x + \\delta), y) + \\lambda \\cdot \\| \\delta \\|_2\n",
    "\\end{align}\n",
    "\n",
    "where $NLL$ is our negative log-likelihood (cross-entropy) classification loss and $\\lambda$ controls the relative weight of our loss terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60f2c0-f459-4a07-a2e5-9a93da9d74fc",
   "metadata": {
    "id": "bd60f2c0-f459-4a07-a2e5-9a93da9d74fc",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "#### Lagrangian Relaxation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a54353-57ea-4fac-a31a-083d45bac94c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97a54353-57ea-4fac-a31a-083d45bac94c",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "# pick a class we want the network to predict given our adversarial input\n",
    "target = torch.tensor([9]).to(device)\n",
    "\n",
    "x = x.reshape(1, 1, 28, 28)\n",
    "\n",
    "# iteratively optimize an additive perturbation to our input\n",
    "delta = torch.zeros_like(x).requires_grad_(True)\n",
    "\n",
    "# \"training\" loop\n",
    "for i in range(1, 101):\n",
    "\n",
    "    # clear gradients each step\n",
    "    delta.grad = None\n",
    "\n",
    "    # log current prediction\n",
    "    if not i % 10:\n",
    "        print(f\"Iteration: {i}; Prediction: {outputs.argmax()}\")\n",
    "\n",
    "    # compute model's class scores given perturbed input (x + delta)\n",
    "    outputs = model(x + delta)\n",
    "\n",
    "    # compute loss: matching target + penalizing perturbation magnitude\n",
    "    loss = torch.nn.functional.nll_loss(outputs, target) + 0.3 * delta.norm()\n",
    "\n",
    "    # compute gradient of loss w.r.t. our perturbation\n",
    "    loss.backward()\n",
    "    grad = delta.grad.detach()\n",
    "\n",
    "    # update our perturbation to descend the loss\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # update with raw gradients\n",
    "        delta -= (grad / (grad.norm() + 1e-12)) * 0.1\n",
    "\n",
    "# plot final prediction\n",
    "plot_mnist(x + delta.detach(), y, model)\n",
    "\n",
    "# plot perturbation (difference between benign and adversarial inputs)\n",
    "plt.title(\"Perturbation\")\n",
    "plt.imshow(delta.detach().cpu().reshape(28, 28).numpy(), cmap=\"gray\")\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c675fd99-2191-45ce-9594-2803b645854d",
   "metadata": {
    "id": "c675fd99-2191-45ce-9594-2803b645854d",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "And with that, we'll take a break from image attacks and move on to the audio domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad2dc8a-c33b-43a5-856d-2dc4afc0024f",
   "metadata": {
    "id": "2ad2dc8a-c33b-43a5-856d-2dc4afc0024f",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "## __3.__ <a name=\"audio\">Audio-Domain Attacks</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66d0fc8-32ba-4a3f-9fea-7a5d102a668a",
   "metadata": {
    "id": "e66d0fc8-32ba-4a3f-9fea-7a5d102a668a",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "Having seen a few basic attacks in the image domain, we'll now pivot to audio. First, we'll swap our MNIST classification task for an analogous AudioMNIST spoken digit recognition task. Then, we'll survey existing audio-domain attacks and examine how they navigate the differences between visual and auditory media."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc5d581-d9ab-4604-b3fa-6a28b5b7d331",
   "metadata": {
    "id": "cfc5d581-d9ab-4604-b3fa-6a28b5b7d331",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "### __3.1__ <a name=\"audiomnist\">A Simple AudioMNIST Classifier</a>\n",
    "\n",
    "The [AudioMNIST dataset](https://github.com/soerenab/AudioMNIST) comprises 30,000 one-second recordings of spoken digits (0-9) from 60 different speakers. Whereas the grayscale images in the MNIST dataset had an inherent two-dimensional spatial structure that we could exploit with a convolutional neural network, __audio is typically stored in a one-dimensional _waveform_ representation__. There are two common ways to deal with this: use _one-dimensional convolutions_ in our neural network, or convert the audio to a _two-dimensional representation_ and apply standard image-domain techniques. We'll opt for the former, using the AudioNet model proposed by [Becker et al.](https://arxiv.org/abs/1807.03418)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f76467-946b-4a4a-ad6e-e9dbb5682ae4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36f76467-946b-4a4a-ad6e-e9dbb5682ae4",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)  # fix random seed\n",
    "\n",
    "# load AudioMNIST dataset (if you opted to download via GitHub, this will cache the dataset as tensors)\n",
    "audiomnist_train, audiomnist_test = load_audiomnist(\"./data/AudioMNIST\")\n",
    "\n",
    "# select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load neural network classifier\n",
    "model = AudioNet()\n",
    "model.to(device)\n",
    "\n",
    "# print model summary\n",
    "summary(model, (1, 16000))\n",
    "\n",
    "# uncomment this to train a model yourself!\n",
    "# train_audiomnist(model, device, audiomnist_train, audiomnist_test, epochs=10)\n",
    "\n",
    "# load pretrained weights\n",
    "model.load_state_dict(\n",
    "    torch.load(\"./utils/adversarial_examples/pretrained/audiomnist_classifier.pt\", map_location=device)\n",
    ")\n",
    "\n",
    "# evaluate model. This may take a while on CPU, and should be ~98%\n",
    "test_audiomnist(model, device, audiomnist_test)\n",
    "\n",
    "# plot and play an AudioMNIST example\n",
    "batch_x, batch_y = next(iter(audiomnist_test))\n",
    "x, y = batch_x[100:101].to(device), batch_y[100:101].to(device)\n",
    "\n",
    "plot_audiomnist(x, y, model)\n",
    "play_audiomnist(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c5c44-7c53-4614-b70e-709959ff906d",
   "metadata": {
    "id": "4f3c5c44-7c53-4614-b70e-709959ff906d",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "### __3.2__ <a name=\"craft-audio\">Crafting an Audio Adversarial Example</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9735483-a2fd-46b3-923c-aa8b69edf38e",
   "metadata": {
    "id": "b9735483-a2fd-46b3-923c-aa8b69edf38e",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "Just as we attacked an image classifier by making small alterations in pixel space, we can attack an audio classifier by making small alterations to the waveform. First, let's see where random noise gets us:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443693db-1b22-4b89-a1fe-73dea5112a01",
   "metadata": {
    "id": "443693db-1b22-4b89-a1fe-73dea5112a01",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "#### Random Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef5b5bf-92db-4573-9763-a30537da53e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ef5b5bf-92db-4573-9763-a30537da53e4",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "# add random noise\n",
    "delta = torch.rand_like(x) * 0.001\n",
    "plot_audiomnist(x + delta, y, model)\n",
    "play_audiomnist(x + delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d162399-ea28-4cc8-9ae5-7994e1368e1c",
   "metadata": {
    "id": "4d162399-ea28-4cc8-9ae5-7994e1368e1c",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "Just as before, we see (or hear) that we may fail to change the model's prediction even when adding a very perceptible amount of random noise. How can we perturb the input more precisely? With gradients!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc31e8-e579-4e83-8b73-ac948d2837ee",
   "metadata": {
    "id": "2fbc31e8-e579-4e83-8b73-ac948d2837ee",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "#### Gradients at the Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e4d9e1-6e28-4837-8efc-f3252486bd45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57e4d9e1-6e28-4837-8efc-f3252486bd45",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "# pick a class we want the network to predict given our adversarial input\n",
    "target = torch.tensor([0]).to(device)\n",
    "\n",
    "# our input audio\n",
    "x = x.detach().reshape(1, 1, 16000).requires_grad_(True)\n",
    "\n",
    "# clear gradients\n",
    "x.grad = None\n",
    "\n",
    "# do not tack gradients for model -- only input\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.eval()\n",
    "\n",
    "# pass through model and compute loss: how \"far\" is prediction from target?\n",
    "outputs = model(x)\n",
    "loss = torch.nn.functional.cross_entropy(outputs, target)\n",
    "\n",
    "# compute gradient of loss w.r.t. our input\n",
    "loss.backward()\n",
    "\n",
    "# plot gradient\n",
    "grad = x.grad.detach().cpu().flatten().numpy()\n",
    "plt.title(\"Gradient\")\n",
    "plt.plot(grad, \"k-\")\n",
    "plt.show()\n",
    "\n",
    "play_audiomnist(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136dd6ac-4108-4be7-a663-dd591bf6542f",
   "metadata": {
    "id": "136dd6ac-4108-4be7-a663-dd591bf6542f",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "Just as before, we can iteratively update our input using these gradients to craft an adversarial example that minimizes our classification loss. Note the large magnitude of these gradients relative to the input waveform. To avoid scale issues, we can use the $L_\\infty$ PGD attack discussed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee053b1-62b1-409d-8aba-db2db55508cd",
   "metadata": {
    "id": "8ee053b1-62b1-409d-8aba-db2db55508cd",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "#### $L_\\infty$ PGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4e6c0-b950-4062-8fdf-8120e612acb5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9c4e6c0-b950-4062-8fdf-8120e612acb5",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "# pick a class we want the network to predict given our adversarial input\n",
    "target = torch.tensor([0]).to(device)\n",
    "\n",
    "x = x.detach().reshape(1, 1, 16000).requires_grad_(False)\n",
    "\n",
    "# iteratively optimize an additive perturbation to our input\n",
    "delta = torch.zeros_like(x).requires_grad_(True)\n",
    "\n",
    "# \"training\" loop\n",
    "for i in range(1, 11):\n",
    "\n",
    "    # clear gradients each step\n",
    "    delta.grad = None\n",
    "\n",
    "    # compute model's class scores given perturbed input (x + delta)\n",
    "    outputs = model(x + delta)\n",
    "\n",
    "    # log current prediction\n",
    "    print(f\"Iteration: {i}; Prediction: {outputs.argmax()}\")\n",
    "\n",
    "    # compute loss: how \"far\" is prediction from target?\n",
    "    loss = torch.nn.functional.cross_entropy(outputs, target)\n",
    "\n",
    "    # compute gradient of loss w.r.t. our perturbation\n",
    "    loss.backward()\n",
    "    grad = delta.grad.detach()\n",
    "\n",
    "    # update our perturbation to descend the loss\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # take uniform steps along the gradient for all samples\n",
    "        delta -= torch.sign(grad) * 0.0001\n",
    "\n",
    "        # bound the magnitude of the perturbation at each iteration\n",
    "        delta.clip_(min=-0.0005, max=0.0005)\n",
    "\n",
    "# plot final prediction\n",
    "plot_audiomnist(x + delta.detach(), y, model)\n",
    "play_audiomnist((x + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2294efff-ea65-4c28-88e9-171038b0ab6b",
   "metadata": {
    "id": "2294efff-ea65-4c28-88e9-171038b0ab6b",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "Note that we had to carefully pick our step size and allow sufficient iterations for our optimization to converge to an effective adversarial example. Also note that while our perturbation is small in magnitude, it is easily perceptible as noise. As before, we can try using an __auxiliary loss (Lagrangian relaxation)__ to penalize the magnitude of our perturbation and make it harder to hear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999b0873-5235-4190-86ab-1e437fe04abf",
   "metadata": {
    "id": "999b0873-5235-4190-86ab-1e437fe04abf",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "#### Lagrangian Relaxation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ddea3-4391-4779-b886-d86fdd0486f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a54ddea3-4391-4779-b886-d86fdd0486f2",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "# pick a class we want the network to predict given our adversarial input\n",
    "target = torch.tensor([0]).to(device)\n",
    "\n",
    "x = x.detach().reshape(1, 1, 16000)\n",
    "\n",
    "# iteratively optimize an additive perturbation to our input\n",
    "delta = torch.zeros_like(x).requires_grad_(True)\n",
    "\n",
    "# \"training\" loop\n",
    "for i in range(1, 201):\n",
    "\n",
    "    # clear gradients each step\n",
    "    delta.grad = None\n",
    "\n",
    "    # compute model's class scores given perturbed input (x + delta)\n",
    "    outputs = model(x + delta)\n",
    "\n",
    "    # log current prediction\n",
    "    if not i % 20:\n",
    "        print(f\"Iteration: {i}; Prediction: {outputs.argmax()}\")\n",
    "\n",
    "    # compute loss: how \"far\" is prediction from target?\n",
    "    loss = torch.nn.functional.cross_entropy(outputs, target) + 400 * delta.norm()\n",
    "\n",
    "    # compute gradient of loss w.r.t. our perturbation\n",
    "    loss.backward()\n",
    "    grad = delta.grad.detach()\n",
    "\n",
    "    # update our perturbation to descend the loss\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # take uniform-magnitude steps along the gradient for all samples\n",
    "        delta -= (grad / (grad.norm() + 1e-12)) * 0.005\n",
    "\n",
    "# plot final prediction\n",
    "plot_audiomnist(x + delta.detach(), y, model)\n",
    "play_audiomnist((x + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04865685-1c91-45b3-9bdf-0065b0f48a2c",
   "metadata": {
    "id": "04865685-1c91-45b3-9bdf-0065b0f48a2c",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "That certainly helps a bit -- our perturbation is less conspicuous than before, although still hard to miss. Interestingly, while small noisy perturbations can be hard to spot in the image domain, they are often quite easy to hear in the audio domain. In order to craft stealthier audio perturbations, we'll have to get creative.\n",
    "\n",
    "In our Lagrangian relaxation above, we used a simple $L_2$ penalty as our auxiliary loss. However, due to the complex nature of human auditory perception, the $L_2$ norm of an audio signal does not necessarily correspond well with its \"perceptibility.\" To improve on this front, we can swap in an auxliary loss designed to mimic the ways in which the human ear is sensitive to sound -- namely, by leveraging the phenomenon of _frequency masking_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f48d8b-fd99-4f18-9e46-fbea17223ebb",
   "metadata": {
    "id": "75f48d8b-fd99-4f18-9e46-fbea17223ebb",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "#### Frequency Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e567ccf-7443-4476-9ae8-e2ab13dc60d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e567ccf-7443-4476-9ae8-e2ab13dc60d6",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "# initialize an object to compute auxiliary (frequency-masking) loss\n",
    "fm_loss = FrequencyMaskingLoss()\n",
    "\n",
    "# store \"clean\" version of waveform as perceptual reference\n",
    "fm_loss.set_reference(x)\n",
    "\n",
    "# pick a class we want the network to predict given our adversarial input\n",
    "target = torch.tensor([0]).to(device)\n",
    "\n",
    "# iteratively optimize an additive perturbation to our input\n",
    "delta = torch.zeros_like(x).requires_grad_(True)\n",
    "\n",
    "# \"training\" loop\n",
    "for i in range(1, 501):\n",
    "\n",
    "    # clear gradients each step\n",
    "    delta.grad = None\n",
    "\n",
    "    # compute model's class scores given perturbed input (x + delta)\n",
    "    outputs = model(x + delta)\n",
    "\n",
    "    # compute losses: how \"far\" is prediction from target? how perceptible is perturbation?\n",
    "    l_adv = torch.nn.functional.cross_entropy(outputs, target)\n",
    "    l_aux = fm_loss(x + delta, x)\n",
    "\n",
    "    # we'll stop optimizing the adversarial loss after we reach a value of 0.05. This will\n",
    "    # let us focus on the auxiliary loss, making our perturbation less perceptible\n",
    "    loss = torch.nn.functional.relu(l_adv - 0.05) + l_aux\n",
    "\n",
    "    # log current prediction\n",
    "    if not i % 100:\n",
    "        print(f\"Iteration: {i}; Prediction: {outputs.argmax()}; L_adv: {l_adv.item()}; L_aux: {l_aux.item()}\")\n",
    "\n",
    "    # compute gradient of loss w.r.t. our perturbation\n",
    "    loss.backward()\n",
    "    grad = delta.grad.detach()\n",
    "\n",
    "    # update our perturbation to descend the loss\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # take uniform steps along the gradient for all samples\n",
    "        delta -= (grad / (grad.norm() + 1e-12)) * 1e-4\n",
    "\n",
    "# plot final prediction\n",
    "plot_audiomnist(x + delta.detach(), y, model)\n",
    "play_audiomnist((x + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d337327-ad3c-4250-be99-5dd21105cc6d",
   "metadata": {
    "id": "5d337327-ad3c-4250-be99-5dd21105cc6d",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "For reference, here's the original (unperturbed) audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6af82ea-0ac3-4aac-ab21-a4d9bda42ee3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6af82ea-0ac3-4aac-ab21-a4d9bda42ee3",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "play_audiomnist(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a810ed3-7e3e-4ae5-ae47-f325b6a7da39",
   "metadata": {
    "id": "5a810ed3-7e3e-4ae5-ae47-f325b6a7da39",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "## __4.__ <a name=\"robust\">Building Robust Models</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e454ba3-3795-40ad-8570-779b1f9ae0f8",
   "metadata": {
    "id": "6e454ba3-3795-40ad-8570-779b1f9ae0f8",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "We've explored a number of ways to craft adversarial examples. While these approaches relied on direct access to the model we were attempting to fool, it turns out that adversarial examples can often __transfer__ to unseen models and have similar effects. Additionally, __black-box__ optimization approaches can allow attackers to craft adversarial examples with only query access to a victim model. These factors mean that adversarial examples may pose a nontrivial risk to machine-learning systems in certain sensitive applications. So how can we prevent malicious actors from surreptitiously influencing the behavior of neural network systems?\n",
    "\n",
    "One __adversarial defense__, called __randomized smoothing__, applies random transformations to many copies of an input and takes a majority vote over the model's predictions on the transformed inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f74b1-9ffe-4437-aac7-61e6d0745ac6",
   "metadata": {
    "id": "8d5f74b1-9ffe-4437-aac7-61e6d0745ac6"
   },
   "source": [
    "#### Randomized Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c41b2a-ebab-4f58-ac3f-8e074970a073",
   "metadata": {
    "id": "c5c41b2a-ebab-4f58-ac3f-8e074970a073",
    "outputId": "9b5b4b87-c7dd-40b5-94e6-702ae41af1ee"
   },
   "outputs": [],
   "source": [
    "# use our frequency-masked perturbation from above\n",
    "adv_input = x + delta\n",
    "\n",
    "print(f\"Original prediction: {model(x).argmax().item()}\")\n",
    "print(f\"Adversarial prediction (no randomized smoothing): {model(adv_input).argmax().item()}\")\n",
    "\n",
    "# generate 100 \"noisy\" versions of our input and get a prediction for each\n",
    "noisy_predictions = []\n",
    "for i in range(100):\n",
    "    noisy_predictions.append(model(adv_input + torch.randn_like(adv_input) * 0.0001).argmax().item())\n",
    "\n",
    "# take most common prediction\n",
    "mode = max(set(noisy_predictions), key=noisy_predictions.count)\n",
    "\n",
    "print(f\"Adversarial prediction (randomized smoothing): {mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd7e82-6022-4e75-902f-a4c2b9af7823",
   "metadata": {
    "id": "e3cd7e82-6022-4e75-902f-a4c2b9af7823"
   },
   "source": [
    "This can work well against small, \"brittle\" adversarial perturbations which will be effectively \"drowned out\" by the noise. However, there are still plenty of issues with this approach. For example, adversaries can include random noise in their optimization process to craft perturbations capable of withstanding noise. Additionally, in order to beat larger perturbations, randomized smoothing must use a correspondingly larger amount of noise at inference time, resulting in inaccurate predictions.\n",
    "\n",
    "Other adversarial defenses attempt to detect malicious inputs using a variety of heuristics. However, these defenses often fail to provide meaningful guarantees against __adaptive__ attackers with knowledge of the defense. Currently, one of the best known approaches to prevent attacks is __adversarial training__, in which a model is trained on adversarial examples. This can provide some degree of robustness against attacks similar to those seen during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ed91c2-df9e-4ba1-92d1-b9557623dbdf",
   "metadata": {
    "id": "25ed91c2-df9e-4ba1-92d1-b9557623dbdf"
   },
   "source": [
    "#### Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d6faf-eee8-4b69-88a7-9e6ffc4585e8",
   "metadata": {
    "id": "134d6faf-eee8-4b69-88a7-9e6ffc4585e8",
    "outputId": "18ff8a86-6669-4447-cddf-ad9e5b6ecf73"
   },
   "outputs": [],
   "source": [
    "# we'll start by wrapping our adversarial example code in a function\n",
    "def make_adversarial_example(x: torch.Tensor, model: torch.nn.Module, steps: int):\n",
    "\n",
    "    # require batch dimension\n",
    "    assert x.ndim >= 2\n",
    "    n_batch = x.shape[0]\n",
    "\n",
    "    x = x.detach().reshape(n_batch, 1, 16000)\n",
    "    delta = torch.zeros_like(x).requires_grad_(True)\n",
    "\n",
    "    # obtain original predictions; we'll perform an `untargeted` attack in which we simply\n",
    "    # seek incorrect predictions rather than a specific class\n",
    "    with torch.no_grad():\n",
    "        preds = model(x).argmax(dim=-1).long()\n",
    "\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(steps):\n",
    "\n",
    "        delta.grad = None\n",
    "\n",
    "        outputs = model(x + delta)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, preds)\n",
    "\n",
    "        loss.backward()\n",
    "        grad = delta.grad.detach()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # ascend rather than descend the gradient (untargeted attack)\n",
    "            delta += (grad / (grad.norm() + 1e-12)) * 0.005\n",
    "\n",
    "    # restore train mode\n",
    "    model.train()\n",
    "\n",
    "    return x + delta.detach()\n",
    "\n",
    "\n",
    "# choose how many attack steps we perform -- more steps means more robustness, but at the\n",
    "# cost of a linear increase in training time\n",
    "attack_steps = 5\n",
    "\n",
    "# number of epochs to train for\n",
    "epochs = 5\n",
    "\n",
    "# initialize the model\n",
    "robust_model = AudioNet()\n",
    "robust_model.to(device)\n",
    "\n",
    "# use an optimizer to handle parameter updates\n",
    "optimizer = torch.optim.SGD(robust_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# train using categorical cross-entropy loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# keep track of accuracy; ultimately, we will choose the most accurate model\n",
    "best_acc = 0.0\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # track loss\n",
    "    training_loss = 0.0\n",
    "    validation_loss = 0\n",
    "\n",
    "    # track accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(audiomnist_train, total=len(audiomnist_train))\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(pbar):\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch + 1}, batch {batch_idx + 1}/{len(audiomnist_train)}\")\n",
    "\n",
    "        inputs, labels = batch_data\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # adversarially perturb inputs\n",
    "        inputs = make_adversarial_example(inputs, robust_model, attack_steps)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = robust_model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # sum training loss\n",
    "        training_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pbar = tqdm(audiomnist_test, total=len(audiomnist_test))\n",
    "        for batch_idx, batch_data in enumerate(pbar):\n",
    "\n",
    "            pbar.set_description(f\"Validation, batch {batch_idx + 1}/{len(audiomnist_test)}\")\n",
    "\n",
    "            inputs, labels = batch_data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = robust_model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # sum validation loss\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            # calculate validation accuracy\n",
    "            preds = torch.max(outputs.data, 1)[1]\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    # calculate final metrics\n",
    "    validation_loss /= len(audiomnist_test)\n",
    "    training_loss /= len(audiomnist_train)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # if best model thus far, save\n",
    "    if accuracy > best_acc:\n",
    "        print(f\"New best accuracy: {accuracy}; saving model\")\n",
    "        best_model = copy.deepcopy(robust_model.state_dict())\n",
    "        best_acc = accuracy\n",
    "\n",
    "# use best weights\n",
    "robust_model.load_state_dict(best_model)\n",
    "\n",
    "# evaluate robust model's accuracy on \"natural\" (non-adversarial) data\n",
    "test_audiomnist(robust_model, device, audiomnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5675781-475b-4d28-b9ad-df2ab3fc9ac5",
   "metadata": {
    "id": "f5675781-475b-4d28-b9ad-df2ab3fc9ac5",
    "outputId": "d080d1d3-fa3e-40ff-9ad0-5830087c443d"
   },
   "outputs": [],
   "source": [
    "# compare predictions!\n",
    "print(f\"Original prediction: {model(x).argmax().item()}\")\n",
    "print(f\"Adversarial prediction (non-robust model): {model(adv_input).argmax().item()}\")\n",
    "print(f\"Adversarial prediction (robust model): {robust_model(adv_input).argmax().item()}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
